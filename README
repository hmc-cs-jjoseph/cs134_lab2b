NAME: Jesse Joseph
EMAIL: jjoseph@hmc.edu
ID: 040161840

ABOUT:
 -On my plot lab2b_3.png, the data points for spin block out the points for mutex because they're in the same place. I know they're there because if I ge rid of the spin data points the mutex points show up.

FILES:
 - lab2_list.c
 - SortedList.h
 - SortedList.c
 - lab2_list.csv
 - add_test.sh - shell script for running add tests
 - list_test.sh - shell script for running list tests

QUESTIONS

Question 2.3.1 - Cycles in the basic list implementation
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests ?
Why do you believe these to be the most expensive parts of the code?
	-With just 1 or 2 threads, there's very little contention. I'd bet that the most cycles are spent in the insert
	operations, because they need to traverse an average of half the list length and compare each key, which will
	be an expensive operation. Length, however, may also be expensive because it needs to traverse the entire
	list.
Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
	-In the spin lock tests with high contention, a huge amount of time is spent spinning. Most threads
	won't be able to run when given the opportunity, and will sit and spin for their entire time.
Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?
	-In the mutex implementation, the blocked threads won't waste time spinning and doing nothing. When a blocked
	thread is brought to the front of the queue, if it cannot take the lock, it won't spin, but it will cause a
	context switch to put the thread back to sleep to wait its turn again. Significant time is probably spent in 
	the context switches.

Question 2.3.2 - Execution profiling
Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?
	- I used linux perf instead of google-pprof. perf samples at a continuous rate, and so the number of samples collected by perf in a given method
	is directly proportional to the time spent in that method. perf shows that the vast majority of the time was spent spinning in the spin lock.
		- In the test I just did, 88% of samples were taken during the routine spinLock
Why does this operation become so expensive with large numbers of threads?
	-This is expensive under high contention because every thread that wakes up blocked sits at this function and executes the expensive atomit test and set until it's out of time.

Question 2.3.3 - Mutex wait time
Look at the average time per operation (vs # threads) and the average wait-for-mutex time (vs #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?
	-The average wait time increases drastically with the number of contending threads because each thread has less of a chance
	of acquiring the lock, forcing each thread to wait longer.
Why does the completion time per operation rise (less dramatically) with the number of contending threads?
	-The completion time increases for the same reason, but it doesn't rise as quickly because the operations are always being completed. 
How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?
	-Wait time increases faster than operation time because the total operation time includes wait time AND the time actually performing code.
	Therefore, the time actually executing the operations, and not just waiting, helps to amortize the wait time in each operation.

QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
	-The throughput of the synchronized methods increases as the number of lists increases.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.
	-In the limit where the number of lists approaches the number of elements, this data structure becomes closer to a standard hash table that avoids collisions using separate chaining.
	The throughput will continue to increase, because more lists results in less contention over locks.
It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.
	-This is not the case in the curves generated.
	If this were true, a 4-way partitioned list with 4 threads would have the same throughput as a single list with 1 thread. 
	An 8-way partitioned list with 8 threads would also have the same throughput, as would a 16-way 16-thread implementation.
	However, the graphs show that the throughput for 4-way 4-thread is slightly higher than 1-way, 1-thread, and 8-way 8-thread is even higher.
	The increase is small, but consistent.
